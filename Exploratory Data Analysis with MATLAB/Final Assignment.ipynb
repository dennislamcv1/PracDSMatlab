{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurricane Harvey \n",
    "\n",
    "In 2017, Hurricane Harvey became one of the costliest hurricanes on record, causing approximately $125 billion in damage. Most of the damage was caused from flooding with some areas receiving over 40 inches (102 cm) of rain.\n",
    "\n",
    "Hurricanes are large weather events with a specific definition – a rotating low-pressure weather system with sustained winds of 74+ mph (119 km/h). Harvey became a hurricane August 24th, made landfall on the 25th, and was downgraded to a tropical storm on August 26th.\n",
    "\n",
    "The impact of Harvey was felt over much more than just 3 days. In the 2017 storm events data set, Harvey related events are reported beginning August 17th and end September 3rd as the system moved north and east across the United States. Flooding, thunderstorms, hail, and tornadoes are just a few of the weather events related to Harvey. \n",
    "\n",
    "One of your tasks as the hired data scientist is to find the two states with the most damage from Harvey related events.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two States Most Impacted by Harvey\n",
    "Clearly state the two states in order "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Events for Two Most Impacted States\n",
    "\n",
    "Create and display a few rows of events that include only the two most affected states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure of Event Types\n",
    "\n",
    "Create a figure showing the type and number of occurances for events related to Harvey in the two states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure of Event Locations\n",
    "\n",
    "Show the location of events in the two states. Be sure to use different markers for the two states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis\n",
    "\n",
    "Three Counties with Most Events in State 1\n",
    "Either type out, show in a table, or show in a clear visualization the three counties with the most events in state 1.\n",
    "\n",
    "Three Counties with Most Events in State 2\n",
    "Either type out, show in a table, or show in a clear visualization the three counties with the most events in state 2.\n",
    "\n",
    "Three Counties with Highest Property Cost in State 1\n",
    "Either type out, show in a table, or show in a clear visualization the three counties with the highest reported property cost in state 1. Be sure to include the dollar amount. \n",
    "\n",
    "Three Counties with Highest Property Cost in State 2\n",
    "Either type out, show in a table, or show in a clear visualization the three counties with the highest reported property cost in state 2. Be sure to include the dollar amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import count_nonzero\n",
    "from numpy import median\n",
    "from numpy import mean\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import random\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import eli5\n",
    "from IPython.display import display\n",
    "\n",
    "#import os\n",
    "#import zipfile\n",
    "import scipy.stats\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, auc, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix, plot_roc_curve\n",
    "\n",
    "%matplotlib inline\n",
    "#sets the default autosave frequency in seconds\n",
    "%autosave 60 \n",
    "sns.set_style('dark')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "plt.rc('axes', titlesize=9)\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Use Feature-Engine library\n",
    "#import feature_engine\n",
    "#from feature_engine import imputation as mdi\n",
    "#from feature_engine.outlier_removers import Winsorizer\n",
    "#from feature_engine import categorical_encoders as ce\n",
    "#from feature_engine.discretisation import EqualWidthDiscretiser, EqualFrequencyDiscretiser\n",
    "#from feature_engine.discretisation import ArbitraryDiscretiser, DecisionTreeDiscretiser\n",
    "#from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "#pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format','{:.2f}'.format)\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"StormEvents_2017_finalProject.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EpisodeID</th>\n",
       "      <th>Event_ID</th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Event_Type</th>\n",
       "      <th>CZ_Name</th>\n",
       "      <th>Begin_Date_Time</th>\n",
       "      <th>Timezone</th>\n",
       "      <th>End_Date_Time</th>\n",
       "      <th>Injuries_Direct</th>\n",
       "      <th>Injuries_Indirect</th>\n",
       "      <th>Deaths_Direct</th>\n",
       "      <th>Deaths_Indirect</th>\n",
       "      <th>Damage_Property</th>\n",
       "      <th>Property_Cost</th>\n",
       "      <th>Damage_Crops</th>\n",
       "      <th>Crop_Cost</th>\n",
       "      <th>Begin_Lat</th>\n",
       "      <th>Begin_Lon</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>End_Lon</th>\n",
       "      <th>Episode_Narrative</th>\n",
       "      <th>Event_Narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>113355</td>\n",
       "      <td>678791</td>\n",
       "      <td>NEW JERSEY</td>\n",
       "      <td>2017</td>\n",
       "      <td>April</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>GLOUCESTER</td>\n",
       "      <td>2017-04-06 15:09:00</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>2017-04-06 15:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.66</td>\n",
       "      <td>-75.08</td>\n",
       "      <td>39.66</td>\n",
       "      <td>-75.08</td>\n",
       "      <td>Low pressure tracked from the Ohio Valley into...</td>\n",
       "      <td>A couple of trees were taken down due to thund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113459</td>\n",
       "      <td>679228</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>2017</td>\n",
       "      <td>April</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>LEE</td>\n",
       "      <td>2017-04-06 09:30:00</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>2017-04-06 09:40:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110.00K</td>\n",
       "      <td>110000.00</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.50</td>\n",
       "      <td>-82.00</td>\n",
       "      <td>26.53</td>\n",
       "      <td>-81.88</td>\n",
       "      <td>A line of thunderstorms developed along a pref...</td>\n",
       "      <td>Emergency management reported and broadcast me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113448</td>\n",
       "      <td>679268</td>\n",
       "      <td>OHIO</td>\n",
       "      <td>2017</td>\n",
       "      <td>April</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>GREENE</td>\n",
       "      <td>2017-04-05 17:49:00</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>2017-04-05 17:53:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00K</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.85</td>\n",
       "      <td>-83.99</td>\n",
       "      <td>39.85</td>\n",
       "      <td>-83.99</td>\n",
       "      <td>Showers and thunderstorms developed ahead of a...</td>\n",
       "      <td>An entire tree was uprooted in a yard on Dayto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113697</td>\n",
       "      <td>682042</td>\n",
       "      <td>OHIO</td>\n",
       "      <td>2017</td>\n",
       "      <td>April</td>\n",
       "      <td>Flood</td>\n",
       "      <td>CLERMONT</td>\n",
       "      <td>2017-04-16 17:59:00</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>2017-04-16 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.00K</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.11</td>\n",
       "      <td>-84.29</td>\n",
       "      <td>39.11</td>\n",
       "      <td>-84.29</td>\n",
       "      <td>Thunderstorms with very heavy rain developed a...</td>\n",
       "      <td>Garage of a home was flooded by high water.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113683</td>\n",
       "      <td>682062</td>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>2017</td>\n",
       "      <td>April</td>\n",
       "      <td>Hail</td>\n",
       "      <td>CASS</td>\n",
       "      <td>2017-04-15 15:50:00</td>\n",
       "      <td>CST-6</td>\n",
       "      <td>2017-04-15 15:50:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.98</td>\n",
       "      <td>-95.89</td>\n",
       "      <td>40.98</td>\n",
       "      <td>-95.89</td>\n",
       "      <td>An upper level storm system moved into Nebrask...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57000</th>\n",
       "      <td>119111</td>\n",
       "      <td>716157</td>\n",
       "      <td>IOWA</td>\n",
       "      <td>2017</td>\n",
       "      <td>July</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>JOHNSON</td>\n",
       "      <td>2017-07-21 17:05:00</td>\n",
       "      <td>CST-6</td>\n",
       "      <td>2017-07-21 20:05:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.68</td>\n",
       "      <td>-91.56</td>\n",
       "      <td>41.65</td>\n",
       "      <td>-91.56</td>\n",
       "      <td>Another round of convective storms fired along...</td>\n",
       "      <td>The county emergency manager reported a few ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57001</th>\n",
       "      <td>117398</td>\n",
       "      <td>715844</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>2017</td>\n",
       "      <td>July</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>FAYETTE</td>\n",
       "      <td>2017-07-28 17:10:00</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>2017-07-29 05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.00K</td>\n",
       "      <td>60000.00</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.90</td>\n",
       "      <td>-79.74</td>\n",
       "      <td>39.91</td>\n",
       "      <td>-79.73</td>\n",
       "      <td>Unusually strong upper low for July dropped fr...</td>\n",
       "      <td>Local 911 reported multiple streets flooded wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57002</th>\n",
       "      <td>117398</td>\n",
       "      <td>715845</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>2017</td>\n",
       "      <td>July</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>GREENE</td>\n",
       "      <td>2017-07-28 17:41:00</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>2017-07-28 23:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00K</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.93</td>\n",
       "      <td>-80.00</td>\n",
       "      <td>39.93</td>\n",
       "      <td>-79.99</td>\n",
       "      <td>Unusually strong upper low for July dropped fr...</td>\n",
       "      <td>Mudslide onto 355 Crucible Rd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57003</th>\n",
       "      <td>117398</td>\n",
       "      <td>715848</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>2017</td>\n",
       "      <td>July</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>FAYETTE</td>\n",
       "      <td>2017-07-28 17:59:00</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>2017-07-28 20:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.00K</td>\n",
       "      <td>20000.00</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.84</td>\n",
       "      <td>-79.61</td>\n",
       "      <td>39.84</td>\n",
       "      <td>-79.61</td>\n",
       "      <td>Unusually strong upper low for July dropped fr...</td>\n",
       "      <td>The public reported cars becoming submerged in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57004</th>\n",
       "      <td>113193</td>\n",
       "      <td>677943</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>2017</td>\n",
       "      <td>January</td>\n",
       "      <td>Winter Weather</td>\n",
       "      <td>LANCASTER</td>\n",
       "      <td>2017-01-07 04:30:00</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>2017-01-07 11:15:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low pressure tracking across the FL Panhandle ...</td>\n",
       "      <td>Lancaster County EM reported up to 1 inch of s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57005 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       EpisodeID  Event_ID           State  Year    Month         Event_Type     CZ_Name      Begin_Date_Time Timezone        End_Date_Time  Injuries_Direct  Injuries_Indirect  Deaths_Direct  Deaths_Indirect Damage_Property  Property_Cost Damage_Crops  Crop_Cost  Begin_Lat  Begin_Lon  End_Lat  End_Lon                                  Episode_Narrative                                    Event_Narrative\n",
       "0         113355    678791      NEW JERSEY  2017    April  Thunderstorm Wind  GLOUCESTER  2017-04-06 15:09:00    EST-5  2017-04-06 15:09:00                0                  0              0                0             NaN            NaN          NaN        NaN      39.66     -75.08    39.66   -75.08  Low pressure tracked from the Ohio Valley into...  A couple of trees were taken down due to thund...\n",
       "1         113459    679228         FLORIDA  2017    April            Tornado         LEE  2017-04-06 09:30:00    EST-5  2017-04-06 09:40:00                1                  0              0                0         110.00K      110000.00        0.00K       0.00      26.50     -82.00    26.53   -81.88  A line of thunderstorms developed along a pref...  Emergency management reported and broadcast me...\n",
       "2         113448    679268            OHIO  2017    April  Thunderstorm Wind      GREENE  2017-04-05 17:49:00    EST-5  2017-04-05 17:53:00                0                  0              0                0           1.00K        1000.00        0.00K       0.00      39.85     -83.99    39.85   -83.99  Showers and thunderstorms developed ahead of a...  An entire tree was uprooted in a yard on Dayto...\n",
       "3         113697    682042            OHIO  2017    April              Flood    CLERMONT  2017-04-16 17:59:00    EST-5  2017-04-16 19:00:00                0                  0              0                0           5.00K        5000.00        0.00K       0.00      39.11     -84.29    39.11   -84.29  Thunderstorms with very heavy rain developed a...        Garage of a home was flooded by high water.\n",
       "4         113683    682062        NEBRASKA  2017    April               Hail        CASS  2017-04-15 15:50:00    CST-6  2017-04-15 15:50:00                0                  0              0                0           0.00K           0.00        0.00K       0.00      40.98     -95.89    40.98   -95.89  An upper level storm system moved into Nebrask...                                                NaN\n",
       "...          ...       ...             ...   ...      ...                ...         ...                  ...      ...                  ...              ...                ...            ...              ...             ...            ...          ...        ...        ...        ...      ...      ...                                                ...                                                ...\n",
       "57000     119111    716157            IOWA  2017     July        Flash Flood     JOHNSON  2017-07-21 17:05:00    CST-6  2017-07-21 20:05:00                0                  0              0                0           0.00K           0.00        0.00K       0.00      41.68     -91.56    41.65   -91.56  Another round of convective storms fired along...  The county emergency manager reported a few ca...\n",
       "57001     117398    715844    PENNSYLVANIA  2017     July        Flash Flood     FAYETTE  2017-07-28 17:10:00    EST-5  2017-07-29 05:00:00                0                  0              0                0          60.00K       60000.00        0.00K       0.00      39.90     -79.74    39.91   -79.73  Unusually strong upper low for July dropped fr...  Local 911 reported multiple streets flooded wi...\n",
       "57002     117398    715845    PENNSYLVANIA  2017     July        Flash Flood      GREENE  2017-07-28 17:41:00    EST-5  2017-07-28 23:00:00                0                  0              0                0           1.00K        1000.00        0.00K       0.00      39.93     -80.00    39.93   -79.99  Unusually strong upper low for July dropped fr...                     Mudslide onto 355 Crucible Rd.\n",
       "57003     117398    715848    PENNSYLVANIA  2017     July        Flash Flood     FAYETTE  2017-07-28 17:59:00    EST-5  2017-07-28 20:00:00                0                  0              0                0          20.00K       20000.00        0.00K       0.00      39.84     -79.61    39.84   -79.61  Unusually strong upper low for July dropped fr...  The public reported cars becoming submerged in...\n",
       "57004     113193    677943  SOUTH CAROLINA  2017  January     Winter Weather   LANCASTER  2017-01-07 04:30:00    EST-5  2017-01-07 11:15:00                0                  0              0                0           0.00K           0.00        0.00K       0.00        NaN        NaN      NaN      NaN  Low pressure tracking across the FL Panhandle ...  Lancaster County EM reported up to 1 inch of s...\n",
       "\n",
       "[57005 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57005 entries, 0 to 57004\n",
      "Data columns (total 24 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   EpisodeID          57005 non-null  int64  \n",
      " 1   Event_ID           57005 non-null  int64  \n",
      " 2   State              57005 non-null  object \n",
      " 3   Year               57005 non-null  int64  \n",
      " 4   Month              57005 non-null  object \n",
      " 5   Event_Type         57005 non-null  object \n",
      " 6   CZ_Name            57005 non-null  object \n",
      " 7   Begin_Date_Time    57005 non-null  object \n",
      " 8   Timezone           57005 non-null  object \n",
      " 9   End_Date_Time      57005 non-null  object \n",
      " 10  Injuries_Direct    57005 non-null  int64  \n",
      " 11  Injuries_Indirect  57005 non-null  int64  \n",
      " 12  Deaths_Direct      57005 non-null  int64  \n",
      " 13  Deaths_Indirect    57005 non-null  int64  \n",
      " 14  Damage_Property    46415 non-null  object \n",
      " 15  Property_Cost      46415 non-null  float64\n",
      " 16  Damage_Crops       46284 non-null  object \n",
      " 17  Crop_Cost          46284 non-null  float64\n",
      " 18  Begin_Lat          39366 non-null  float64\n",
      " 19  Begin_Lon          39366 non-null  float64\n",
      " 20  End_Lat            39366 non-null  float64\n",
      " 21  End_Lon            39366 non-null  float64\n",
      " 22  Episode_Narrative  57005 non-null  object \n",
      " 23  Event_Narrative    44424 non-null  object \n",
      "dtypes: float64(6), int64(7), object(11)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EpisodeID</th>\n",
       "      <th>Event_ID</th>\n",
       "      <th>Year</th>\n",
       "      <th>Injuries_Direct</th>\n",
       "      <th>Injuries_Indirect</th>\n",
       "      <th>Deaths_Direct</th>\n",
       "      <th>Deaths_Indirect</th>\n",
       "      <th>Property_Cost</th>\n",
       "      <th>Crop_Cost</th>\n",
       "      <th>Begin_Lat</th>\n",
       "      <th>Begin_Lon</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>End_Lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>57005.00</td>\n",
       "      <td>57005.00</td>\n",
       "      <td>57005.00</td>\n",
       "      <td>57005.00</td>\n",
       "      <td>57005.00</td>\n",
       "      <td>57005.00</td>\n",
       "      <td>57005.00</td>\n",
       "      <td>46415.00</td>\n",
       "      <td>46284.00</td>\n",
       "      <td>39366.00</td>\n",
       "      <td>39366.00</td>\n",
       "      <td>39366.00</td>\n",
       "      <td>39366.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>116590.53</td>\n",
       "      <td>700184.63</td>\n",
       "      <td>2017.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2297800.43</td>\n",
       "      <td>41336.46</td>\n",
       "      <td>37.61</td>\n",
       "      <td>-90.38</td>\n",
       "      <td>37.61</td>\n",
       "      <td>-90.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2904.65</td>\n",
       "      <td>17735.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.13</td>\n",
       "      <td>111258427.65</td>\n",
       "      <td>1681818.84</td>\n",
       "      <td>5.08</td>\n",
       "      <td>11.51</td>\n",
       "      <td>5.08</td>\n",
       "      <td>11.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>111354.00</td>\n",
       "      <td>664501.00</td>\n",
       "      <td>2017.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-14.36</td>\n",
       "      <td>-170.84</td>\n",
       "      <td>-14.30</td>\n",
       "      <td>-170.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>114130.00</td>\n",
       "      <td>685249.00</td>\n",
       "      <td>2017.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>34.48</td>\n",
       "      <td>-96.82</td>\n",
       "      <td>34.48</td>\n",
       "      <td>-96.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>116237.00</td>\n",
       "      <td>700218.00</td>\n",
       "      <td>2017.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>38.01</td>\n",
       "      <td>-89.86</td>\n",
       "      <td>38.02</td>\n",
       "      <td>-89.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>118944.00</td>\n",
       "      <td>715073.00</td>\n",
       "      <td>2017.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.16</td>\n",
       "      <td>-81.85</td>\n",
       "      <td>41.16</td>\n",
       "      <td>-81.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>127724.00</td>\n",
       "      <td>798846.00</td>\n",
       "      <td>2017.00</td>\n",
       "      <td>56.00</td>\n",
       "      <td>500.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10000000000.00</td>\n",
       "      <td>255000000.00</td>\n",
       "      <td>60.47</td>\n",
       "      <td>-64.74</td>\n",
       "      <td>60.50</td>\n",
       "      <td>-64.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       EpisodeID  Event_ID     Year  Injuries_Direct  Injuries_Indirect  Deaths_Direct  Deaths_Indirect  Property_Cost    Crop_Cost  Begin_Lat  Begin_Lon  End_Lat  End_Lon\n",
       "count   57005.00  57005.00 57005.00         57005.00           57005.00       57005.00         57005.00       46415.00     46284.00   39366.00   39366.00 39366.00 39366.00\n",
       "mean   116590.53 700184.63  2017.00             0.02               0.01           0.01             0.00     2297800.43     41336.46      37.61     -90.38    37.61   -90.37\n",
       "std      2904.65  17735.15     0.00             0.58               2.11           0.23             0.13   111258427.65   1681818.84       5.08      11.51     5.08    11.50\n",
       "min    111354.00 664501.00  2017.00             0.00               0.00           0.00             0.00           0.00         0.00     -14.36    -170.84   -14.30  -170.69\n",
       "25%    114130.00 685249.00  2017.00             0.00               0.00           0.00             0.00           0.00         0.00      34.48     -96.82    34.48   -96.81\n",
       "50%    116237.00 700218.00  2017.00             0.00               0.00           0.00             0.00           0.00         0.00      38.01     -89.86    38.02   -89.85\n",
       "75%    118944.00 715073.00  2017.00             0.00               0.00           0.00             0.00        1000.00         0.00      41.16     -81.85    41.16   -81.84\n",
       "max    127724.00 798846.00  2017.00            56.00             500.00          36.00            20.00 10000000000.00 255000000.00      60.47     -64.74    60.50   -64.74"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas-Profiling Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df=df, title='Name of Report', minimal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(\"your_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=50, figsize=(20,10))\n",
    "plt.suptitle('Histogram Feature Distribution', x=0.5, y=1.02, ha='center', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(figsize=(20,10))\n",
    "plt.suptitle('BoxPlots Feature Distribution', x=0.5, y=1.02, ha='center', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4 rows and 1 column (can be expanded)\n",
    "\n",
    "fig, ax = plt.subplots(4,1, sharex=False, figsize=(16,16))\n",
    "fig.suptitle('Main Title')\n",
    "\n",
    "\n",
    "sns.barplot(x=\"\", y=\"\", data=df, ax=ax[0])\n",
    "ax[0].set_title('Title of the first chart')\n",
    "#ax[0].tick_params('x', labelrotation=45)\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "\n",
    "sns.barplot(x=\"\", y=\"\", data=df, ax=ax[1])\n",
    "ax[1].set_title('Title of the second chart')\n",
    "#ax[1].tick_params('x', labelrotation=45)\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "\n",
    "sns.barplot(x=\"\", y=\"\", data=df, ax=ax[2])\n",
    "ax[2].set_title('Title of the third chart')\n",
    "#ax[2].tick_params('x', labelrotation=45)\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "\n",
    "sns.barplot(x=\"\", y=\"\", data=df, ax=ax[3])\n",
    "ax[3].set_title('Title of the fourth chart')\n",
    "#ax[3].tick_params('x', labelrotation=45)\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1 rows and 2 columns (can be expanded)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharex=False, figsize=(16,5))\n",
    "fig.suptitle('Main Title')\n",
    "\n",
    "sns.countplot(x=\"\", data=df, hue=, ax=ax[0])\n",
    "ax[0].set_title('Title of the first chart')\n",
    "ax[0].tick_params('x', labelrotation=45)\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "\n",
    "sns.countplot(x=\"\", data=df, hue=, ax=ax[1])\n",
    "ax[1].set_title('Title of the second chart')\n",
    "ax[1].tick_params('x', labelrotation=45)\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2 by 2 subplots\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, sharex=False, figsize=(20,20))\n",
    "fig.suptitle('Main Title', y=0.5)\n",
    "\n",
    "sns.countplot(x=\"\", data=df, ax=ax1)\n",
    "ax1.set_title('Title of the first chart', size=20)\n",
    "#ax1.tick_params('x', labelrotation=45)\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_ylabel(\"\")\n",
    "\n",
    "sns.countplot(x=\"\", data=df, ax=ax2)\n",
    "ax2.set_title('Title of the second chart', size=20)\n",
    "#ax2.tick_params('x', labelrotation=45)\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_ylabel(\"\")\n",
    "\n",
    "sns.countplot(x=\"\", data=df, ax=ax3)\n",
    "ax3.set_title('Title of the third chart', size=20)\n",
    "#ax3.tick_params('x', labelrotation=45)\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_ylabel(\"\")\n",
    "\n",
    "sns.countplot(x=\"\", data=df, ax=ax4)\n",
    "ax4.set_title('Title of the fourth chart', size=20)\n",
    "#ax4.tick_params('x', labelrotation=45)\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,40))\n",
    "\n",
    "plt.subplot(7,2,1)\n",
    "plt.title(\"\", size=20)\n",
    "sns.countplot()\n",
    "\n",
    "plt.subplot(7,2,2)\n",
    "plt.title(\"\", size=20)\n",
    "sns.countplot()\n",
    "\n",
    "plt.subplot(7,2,3)\n",
    "plt.title(\"\", size=20)\n",
    "sns.countplot()\n",
    "\n",
    "plt.subplot(7,2,4)\n",
    "plt.title(\"\", size=20)\n",
    "sns.countplot()\n",
    "\n",
    "plt.subplot(7,2,5)\n",
    "plt.title(\"\", size=20)\n",
    "sns.barplot()\n",
    "\n",
    "plt.subplot(7,2,6)\n",
    "plt.title(\"\", size=20)\n",
    "sns.barplot()\n",
    "\n",
    "plt.subplot(7,2,7)\n",
    "plt.title(\"\", size=20)\n",
    "sns.barplot()\n",
    "\n",
    "plt.subplot(7,2,8)\n",
    "plt.title(\"\", size=20)\n",
    "sns.barplot()\n",
    "\n",
    "plt.subplot(7,2,9)\n",
    "plt.title(\"\", size=20)\n",
    "sns.scatterplot()\n",
    "\n",
    "plt.subplot(7,2,10)\n",
    "plt.title(\"\", size=20)\n",
    "sns.scatterplot()\n",
    "\n",
    "plt.subplot(7,2,11)\n",
    "plt.title(\"\", size=20)\n",
    "sns.scatterplot()\n",
    "\n",
    "plt.subplot(7,2,12)\n",
    "plt.title(\"\", size=20)\n",
    "sns.scatterplot()\n",
    "\n",
    "plt.subplot(7,2,13)\n",
    "plt.title(\"\", size=20)\n",
    "sns.relplot()\n",
    "\n",
    "plt.subplot(7,2,14)\n",
    "plt.title(\"\", size=20)\n",
    "sns.relplot()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "\n",
    "g = sns.catplot(x='', hue = '', row = '',\n",
    "            kind='count', data=ratings_df,\n",
    "            height = 3, aspect = 1)\n",
    "\n",
    "g.set_xlabels(\"\")\n",
    "g.set_ylabels(\"\")\n",
    "#g = (g.set_axis_labels(\"Tip\",\"Total bill(USD)\").set(xlim=(0,10),ylim=(0,100)\n",
    "\n",
    "\n",
    "g.set(xlim=(0,None))\n",
    "g.set_xticklabels(rotation=90)\n",
    "\n",
    "plt.suptitle('', x=0.5, y=1.02, ha='center', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "sns.catplot(x=\"calories\", y=\"restaurant\",\n",
    "\n",
    "                hue=\"is_salad\", ci=None,\n",
    "\n",
    "                data=df_calories, color=None, linewidth=3, showfliers = False,\n",
    "\n",
    "                orient=\"h\", height=20, aspect=1, palette=None,\n",
    "\n",
    "                kind=\"box\", dodge=True)\n",
    "\n",
    "plt.xlabel(\"\", size=20)\n",
    "plt.ylabel(\"\", size=20)\n",
    "plt.suptitle('', x=0.5, y=1.02, ha='center', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "sns.relplot(x=\"age\", y=\"eval\", hue=\"gender\",\n",
    "            row=\"tenure\",\n",
    "            data=ratings_df, height = 3, aspect = 2)\n",
    "\n",
    "plt.xlabel(\"\", size=20)\n",
    "plt.ylabel(\"\", size=20)\n",
    "plt.suptitle('', x=0.5, y=1.02, ha='center', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = df[['date','extraction','month', 'day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,10))\n",
    "sns.lineplot(x=df.date,y=df.amount,data=df, estimator=None)\n",
    "plt.title(\"\", fontsize=20)\n",
    "plt.xlabel(\"\", fontsize=20)\n",
    "plt.ylabel(\"\", fontsize=20)\n",
    "plt.legend(['',''])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,10))\n",
    "sns.lineplot(x=df.month,y=df.amount,data=df, estimator=None)\n",
    "plt.title(\"\", fontsize=20)\n",
    "plt.xlabel(\"\", fontsize=20)\n",
    "plt.ylabel(\"\", fontsize=20)\n",
    "plt.legend(['',''])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,10))\n",
    "sns.lineplot(x=df.month,y=df.amount,data=df, estimator=None)\n",
    "plt.title(\"\", fontsize=20)\n",
    "plt.xlabel(\"\", fontsize=20)\n",
    "plt.ylabel(\"\", fontsize=20)\n",
    "plt.legend(['',''])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.suptitle('Pairplots of features', x=0.5, y=1.02, ha='center', fontsize=20)\n",
    "sns.pairplot(df.sample(500))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='', y='',data=df, kind='scatter')\n",
    "\n",
    "sns.jointplot(x='', y='',data=df, kind='scatter')\n",
    "\n",
    "sns.jointplot(x='', y='',data=df, kind='scatter')\n",
    "\n",
    "sns.jointplot(x='', y='',data=df, kind='scatter')\n",
    "\n",
    "sns.jointplot(x='', y='',data=df, kind='kde')\n",
    "\n",
    "sns.jointplot(x='', y='',data=df, kind='kde')\n",
    "\n",
    "sns.jointplot(x='', y='',data=df, kind='hex')\n",
    "\n",
    "sns.jointplot(x='', y='',data=df, kind='hex')\n",
    "\n",
    "sns.jointplot(x='', y='',data=df, kind='reg',scatter_kws={'color':'k'},line_kws={'color':'red'})\n",
    "\n",
    "sns.jointplot(x='', y='',data=df, kind='reg',scatter_kws={'color':'k'},line_kws={'color':'red'})\n",
    "\n",
    "sns.lmplot(x='num_items', y='total_value', data=df, scatter_kws={'s': 1, 'alpha': 0.1}, height=5, aspect=1,\n",
    "           line_kws={'lw': 2, 'color': 'red'})\n",
    "\n",
    "sns.lmplot(x='num_items', y='total_value', data=df, scatter_kws={'s': 1, 'alpha': 0.1}, height=5, aspect=1,\n",
    "           line_kws={'lw': 2, 'color': 'red'})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_color = {'color': 'red'}\n",
    "fig , ax = plt.subplots(2,2, figsize=(20,20))\n",
    "\n",
    "#Feature\n",
    "\n",
    "ax1 = sns.regplot(x=X_test.bmi, y=lr_pred, line_kws=line_color, ax=ax[0,0])\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.set_title(\"Plot 1\", size=15)\n",
    "\n",
    "#Feature\n",
    "\n",
    "ax2 = sns.regplot(x=X_test.s5, y=lr_pred, line_kws=line_color, ax=ax[0,1])\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"y\")\n",
    "ax2.set_title(\"Plot 2\", size=15)\n",
    "\n",
    "#Feature\n",
    "\n",
    "ax3 = sns.regplot(x=X_test.bp, y=lr_pred, line_kws=line_color, ax=ax[1,0])\n",
    "ax3.set_xlabel(\"x\")\n",
    "ax3.set_ylabel(\"y\")\n",
    "ax3.set_title(\"Plot 3\", size=15)\n",
    "\n",
    "#Feature\n",
    "\n",
    "ax4 = sns.regplot(x=X_test.s4, y=lr_pred, line_kws=line_color, ax=ax[1,1])\n",
    "ax4.set_xlabel(\"x\")\n",
    "ax4.set_ylabel(\"y\")\n",
    "ax1.set_title(\"Plot 4\", size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FacetGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=df, col=\"column_name\", height=3, aspect=1)\n",
    "g.map(plt.scatter, \"numeric\", \"numeric\")\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = usa_stores[['City','Latitude','Longtitude','Sentiment','Revenue']]\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[37.090240,-95.712891], zoom_start=5)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df = pd.DataFrame(mapping.groupby([\"City\",\"Latitude\",\"Longtitude\"]).mean())\n",
    "map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.Marker(location=[33.76,-84.42], popup=\"Atlanta\", tooltip=\"Sentiment=83.69, Revenue=292.57\").add_to(m)\n",
    "folium.Marker(location=[36.23,-115.27], popup=\"Las Vegas\", tooltip=\"Sentiment=83.72, Revenue=187.40\").add_to(m)\n",
    "folium.Marker(location=[34.11,-118.41], popup=\"Los Angeles\", tooltip=\"Sentiment=83.75, Revenue=255.95\").add_to(m)\n",
    "folium.Marker(location=[40.69,-73.92], popup=\"New York\", tooltip=\"Sentiment=83.71, Revenue=328.38\").add_to(m)\n",
    "folium.Marker(location=[32.83,-117.12], popup=\"San Diego\", tooltip=\"Sentiment=83.70, Revenue=272.93\").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save(\"filename.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_geo = f\"malaysia.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map2 = folium.Map(location=[4.210484,108.975766], zoom_start=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to create a `Choropleth` map, we will use the *choropleth* method with the following main parameters:\n",
    "\n",
    "1.  `geo_data`, which is the GeoJSON file.\n",
    "2.  `data`, which is the dataframe containing the data.\n",
    "3.  `columns`, which represents the columns in the dataframe that will be used to create the `Choropleth` map.\n",
    "4.  `key_on`, which is the key or variable in the GeoJSON file that contains the name of the variable of interest. To determine that, you will need to open the GeoJSON file using any text editor and note the name of the key or variable that contains the name of the countries, since the countries are our variable of interest. In this case, **name** is the key in the GeoJSON file that contains the name of the countries. Note that this key is case_sensitive, so you need to pass exactly as it exists in the GeoJSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.Choropleth(geo_data=state_geo, name=\"choropleth\").add_to(map2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()[\"target\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.heatmap(df.corr(),cmap=\"coolwarm\",annot=True,fmt='.2f',linewidths=2)\n",
    "plt.title(\"\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of hypothesis testing is to answer the question, “Given a sample and an apparent effect, what is the probability of seeing such an effect by chance?” The first step is to quantify the size of the apparent effect by choosing a test statistic (t-test, ANOVA, etc). The next step is to define a null hypothesis, which is a model of the system based on the assumption that the apparent effect is not real. Then compute the p-value, which is the probability of the null hypothesis being true, and finally interpret the result of the p-value, if the value is low, the effect is said to be statistically significant, which means that the null hypothesis may not be accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the t-test for independent samples. For the independent t-test, the following assumptions must be met.\n",
    "\n",
    "-   One independent, categorical variable with two levels or group\n",
    "-   One dependent continuous variable\n",
    "-   Independence of the observations. Each subject should belong to only one group. There is no relationship between the observations in each group.\n",
    "-   The dependent variable must follow a normal distribution\n",
    "-   Assumption of homogeneity of variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State the hypothesis\n",
    "\n",
    "-   $H_0: µ\\_1 = µ\\_2$ (\"there is no difference in evaluation scores between male and females\")\n",
    "-   $H_1: µ\\_1 ≠ µ\\_2$ (\"there is a difference in evaluation scores between male and females\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levene's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.levene(ratings_df[ratings_df['gender'] == 'female']['eval'],\n",
    "                   ratings_df[ratings_df['gender'] == 'male']['eval'], center='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Sample T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p = scipy.stats.ttest_1samp(a=df.dose, popmean=1.166667)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"T-test value is: \", t)\n",
    "print(\"p-value value is: \", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Samples T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p = scipy.stats.ttest_ind(a=df.len,b=df.dose, equal_var = True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"T-test value is: \",t)\n",
    "print(\"p-value value is: \",p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we group the data into cateries as the one-way ANOVA can't work with continuous variable - using the example from the video, we will create a new column for this newly assigned group our categories will be teachers that are:\n",
    "\n",
    "-   40 years and younger\n",
    "-   between 40 and 57 years\n",
    "-   57 years and older\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State the hypothesis\n",
    "\n",
    "-   $H_0: µ\\_1 = µ\\_2 = µ\\_3$ (the three population means are equal)\n",
    "-   $H_1:$ At least one of the means differ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ols('len~supp', data=df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aov_table = sm.stats.anova_lm(mod,typ=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aov_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_statistic, p_value = scipy.stats.f_oneway(forty_lower, forty_fiftyseven, fiftyseven_older)\n",
    "print(\"F_Statistic: {0}, P-Value: {1}\".format(f_statistic,p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = ols('len~supp+dose', data=df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aov1 = sm.stats.anova_lm(mod1,typ=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aov1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State the hypothesis:\n",
    "\n",
    "-   $H_0:$ The proportion of teachers who are tenured is independent of gender\n",
    "-   $H_1:$ The proportion of teachers who are tenured is associated with gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Cross-tab table\n",
    "\n",
    "cont_table  = pd.crosstab(ratings_df['tenure'], ratings_df['gender'])\n",
    "cont_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.chi2_contingency(cont_table, correction = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square = scipy.stats.chi2_contingency(cont_table, correction = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Chi score is\", chi_square[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"P-value is\", chi_square[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Degrees of freedom is\", chi_square[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State the hypothesis:\n",
    "\n",
    "-   $H_0:$ Teaching evaluation score is not correlated with beauty score\n",
    "-   $H_1:$ Teaching evaluation score is correlated with beauty score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_correlation = scipy.stats.pearsonr(ratings_df['beauty'], ratings_df['eval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pearson's correlation coefficient is\", pearson_correlation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"P-value is\", pearson_correlation[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Width Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"demoscorecat\"] = df[\"polityscore\"] #Make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = EqualWidthDiscretiser(bins=4, variables=['demoscorecat'], return_object=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.binner_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = disc.fit_transform(df)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"demoscorecat\"].value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Frequency Discretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"co2cat\"] = df2[\"co2emissions\"] #Make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = EqualFrequencyDiscretiser(q=5, variables=['co2cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.binner_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = disc.transform(df2)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"co2cat\"].value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretisation + OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose which columns to be discretized first\n",
    "df3[\"incomecat\"] = df3[\"incomeperperson\"] #Make a copy\n",
    "df3[\"alccat\"] = df3[\"alcconsumption\"] #Make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to encode variables we need them returned as objects for feature-engine\n",
    "disc = EqualFrequencyDiscretiser(q=5, variables=['incomecat','alccat'], return_object=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = disc.fit_transform(df3)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[\"incomecat\"].value_counts().plot.bar()\n",
    "df4[\"alccat\"].value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set y = target variable, and x = independant variables (both must be objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4[['demoscorecat','incomecat', 'alccat']]\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.groupby('incomecat')['demoscorecat'].mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.groupby('alccat')['demoscorecat'].mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder(encoding_method = 'ordered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df5[['incomecat', 'alccat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df5['demoscorecat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = enc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.encoder_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform  # Transformed for monotonic relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_transform, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_transform, y], axis=1).groupby('incomecat')['demoscorecat'].mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretisation with Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['electricat'] = df4['relectricperperson'] #Make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let y = demoscorecat, and x = electricat, breastcancerper100th\n",
    "\n",
    "df6 = df4[['breastcancerper100th','electricat','demoscorecat']]\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df6[['breastcancerper100th','electricat']]\n",
    "y = df6['demoscorecat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the decision tree discretiser indicating:\n",
    "# cross-validation number (cv)\n",
    "# how to evaluate model performance (scoring)\n",
    "# the variables we want to discretise (variables)\n",
    "# whether it is a target for regression or classification\n",
    "# and the grid with the parameters we want to test\n",
    "\n",
    "treeDisc = DecisionTreeDiscretiser(cv=5, scoring='accuracy', variables=['electricat'], regression=False,\n",
    "                                  param_grid={'max_depth':[1,2,3], 'min_samples_leaf':[2,4,6]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeDisc.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeDisc.binner_dict_['electricat'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeDisc.scores_dict_['electricat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = treeDisc.transform(X) #Only electricat column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform.electricat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monotonic relationship with target: train set\n",
    "\n",
    "pd.concat([X_transform, y],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unwanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[''] = df[''].replace(np.nan,df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputer = mdi.MeanMedianImputer(imputation_method='median',variables=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = imputer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rounding Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###pandas.DataFrame.round\n",
    "df[['internetuserate']] = df[['internetuserate']].round(decimals=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(keep='first').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(keep=False)] #Check duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#windsorizer = Winsorizer(distribution='skewed',tail='both',fold=1.5, variables=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#windsorizer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = windsorizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#windsorizer.left_tail_caps_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#windsorizer.right_tail_caps_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"breastcancerper100th\"] = df[\"breastcancerper100th\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"has_gas\"] = pd.get_dummies(data=df[\"has_gas\"],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"filename.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (StatsModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['ExpirationMonth']\n",
    "X = df['NumStores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y,X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = smf.ols(formula='Lottery ~ Literacy + Wealth + Region', data=df).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, 'x_variables', fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = sm.graphics.plot_ccpr(prestige_model, \"education\")\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (SKLearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (StatsModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['']\n",
    "X = df['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Logit(y, X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitfit = smf.logit(formula = 'DF ~ Debt_Service_Coverage + cash_security_to_curLiab + TNW', data = hgc).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitfit = smf.logit(formula = 'DF ~ TNW + C(seg2)', data = hgcdev).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (SKLearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:4]\n",
    "y = df.iloc[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train), Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(estimator=lr, X=X_test, y_true=y_test, cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(estimator=lr, X=X_test, y=y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python code done by Dennis Lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
